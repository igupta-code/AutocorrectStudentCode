Time Complexity Analysis:
Constructor:
The time-consuming step here is going through the dictionary to create all the n-grams for your dictionary hash map. Each word can be hashed in time
proportional to it's length (O(1) for the initial hash and then O(1) for each shift over). So the constructor has an O(n*m) complexity, where n is
the average length of all the words in your dictionary and m is the number of words in your dictionary (n will be around 4-5 and m is size 143092
for the large dictionary).

Finding candidates:
Because of our initial set-up, finding our candidate words is pretty quick. We just have to find all the three letter hashes for our misspelled word
and use those hashes to look up words that share those three letters in our hash map. I couldn't figure our a better way to deal with small words than
just go through all of them. So if your word is close enough to the threshold number, you will also have to look through all the 1, 2, and 3 letter
words.

EditDistances:
I use a tabulation dynamic programing approach to find the editDistance of words. This also uses an O(n*m) space and time complexity where n and m are
the lengths of the words being compared. This method is run for the length of candidate words (ones with the a matching sub-string).

Remove Duplicates:
My remove duplicated used to be O(n^2), but Tony and chatGPT helped me come to an  O(n) solution. I was looking at Tony's solution, which from what I
understood, hashed each element and took advantage of collisions to remove duplicates. ChatGPT told me that java has a pre-written way to do a similar
thing.